{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0ae7750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import mlcroissant as mlc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "# feature engineering\n",
    "import tldextract\n",
    "import Levenshtein\n",
    "\n",
    "# logistic regression from sklearn\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import KFold, cross_val_predict, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, mean_squared_error, r2_score,\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_curve, auc, classification_report\n",
    ")\n",
    "\n",
    "# svm and decision tree from sklearn\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import math\n",
    "\n",
    "# XGBoost and Random Forest\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Neural network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, Input, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization, concatenate\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733e3379",
   "metadata": {},
   "source": [
    "### Load Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ad5a0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:WARNING: The JSON-LD `@context` is not standard. Refer to the official @context (e.g., from the example datasets in https://github.com/mlcommons/croissant/tree/main/datasets/1.0). The different keys are: {'rai', 'examples', 'samplingRate'}\n",
      "WARNING:absl:Found the following 1 warning(s) during the validation:\n",
      "  -  [Metadata(Malicious_URL's_Dataset)] Property \"http://mlcommons.org/croissant/citeAs\" is recommended, but does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RecordSet(uuid=\"malicious_phish1.csv\")]\n"
     ]
    }
   ],
   "source": [
    "# Fetch the Croissant JSON-LD\n",
    "croissant_dataset = mlc.Dataset('https://www.kaggle.com/datasets/naveenbhadouria/malicious/croissant/download')\n",
    "\n",
    "# Check what record sets are in the dataset\n",
    "record_sets = croissant_dataset.metadata.record_sets\n",
    "print(record_sets)\n",
    "\n",
    "# Fetch the records and put them in a DataFrame\n",
    "record_set_df = pd.DataFrame(croissant_dataset.records(record_set=record_sets[0].uuid))\n",
    "df = record_set_df\n",
    "\n",
    "# Rename columns\n",
    "df = df.rename(columns={'malicious_phish1.csv/url' : 'url', 'malicious_phish1.csv/type' : 'type'})\n",
    "\n",
    "# Use errors='replace' to substitute bad bytes with a placeholder\n",
    "df['url'] = df['url'].apply(lambda x: x.decode('utf-8', errors='replace') if isinstance(x, bytes) else x)\n",
    "df['type'] = df['type'].apply(lambda x: x.decode('utf-8', errors='replace') if isinstance(x, bytes) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3448f5",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "##### Using a function is better for the UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7212692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_url_features(url):\n",
    "    # Ensure the url is a string\n",
    "    url = str(url)\n",
    "    \n",
    "    # Count based features\n",
    "    features = {\n",
    "        'url_length': len(url),\n",
    "        'num_digits': sum(c.isdigit() for c in url),\n",
    "        'num_periods': url.count('.'),\n",
    "        'num_slashes': url.count('/'),\n",
    "        'num_ats': url.count('@'),\n",
    "        'digit_len_ratio': sum(c.isdigit() for c in url) / len(url) if len(url) > 0 else 0,\n",
    "        'entropy': calc_entropy(url) # Using your existing entropy function\n",
    "    }\n",
    "    \n",
    "    # Boolean features\n",
    "    patterns = {\n",
    "        'has_html': r'\\.html?',\n",
    "        'has_query_param': r'\\?query=',\n",
    "        'has_https': r'^https://',\n",
    "        'has_http': r'^http://',\n",
    "        'has_ip_address': r'://(?:\\d{1,3}\\.){3}\\d{1,3}',\n",
    "        'has_non_ascii_chars': r'[^\\x00-\\x7F]'\n",
    "    }\n",
    "    \n",
    "    for name, pattern in patterns.items():\n",
    "        features[name] = int(bool(re.search(pattern, url, re.IGNORECASE)))\n",
    "        \n",
    "    # Suspicious keyword anywhere in url\n",
    "    suspicious_kw = ['login', 'secure', 'payment', 'verify']\n",
    "    features['has_suspicious_kw'] = int(any(kw in url.lower() for kw in suspicious_kw))\n",
    "    \n",
    "    brands = ['google', 'paypal', 'microsoft', 'apple', 'amazon', 'netflix', 'facebook']\n",
    "    features['has_brand_kw'] = int(any(brand in url.lower() for brand in brands))\n",
    "    \n",
    "    # Domain and typosquat\n",
    "    extracted = tldextract.extract(url)\n",
    "    domain = extracted.domain\n",
    "    features['tld'] = extracted.suffix # We need this for the one hot encoding later\n",
    "    \n",
    "    min_dist = 100\n",
    "    for brand in brands:\n",
    "        dist = Levenshtein.distance(domain.lower(), brand)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            \n",
    "    features['min_brand_dist'] = min_dist\n",
    "    \n",
    "    # Typosquat logic\n",
    "    if min_dist == 0:\n",
    "        features['is_typosquat'] = 0\n",
    "    elif 0 < min_dist <= 2:\n",
    "        features['is_typosquat'] = 1\n",
    "    else:\n",
    "        features['is_typosquat'] = 0\n",
    "        \n",
    "    return features\n",
    "\n",
    "# Calculate Shannon entropy - measures how random the url is (if its gibberish its probably bad)\n",
    "def calc_entropy(text):\n",
    "    if not text: return 0\n",
    "    entropy = 0\n",
    "    for x, n in Counter(str(text)).items():\n",
    "        p = n / len(text)\n",
    "        entropy -= p * math.log2(p)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a6ce1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating features...\n"
     ]
    }
   ],
   "source": [
    "# 1. GENERATE FEATURES FOR ALL DATA\n",
    "print(\"Generating features...\")\n",
    "# Apply the function we created in Step 1\n",
    "features_list = df['url'].apply(extract_url_features).tolist()\n",
    "df_features = pd.DataFrame(features_list)\n",
    "\n",
    "# 2. HANDLE TLDs (Crucial Step for consistency)\n",
    "# Identify top 20 TLDs from training data\n",
    "top_tlds = df_features['tld'].value_counts().nlargest(20).index.tolist()\n",
    "\n",
    "# Function to handle TLD columns manually\n",
    "def process_tlds(tld_series, top_tlds_list):\n",
    "    # Create a DataFrame of 0s\n",
    "    tld_df = pd.DataFrame(0, index=tld_series.index, columns=[f'tld_{t}' for t in top_tlds_list])\n",
    "    tld_df['tld_other'] = 0\n",
    "    \n",
    "    for idx, tld in tld_series.items():\n",
    "        col_name = f'tld_{tld}'\n",
    "        if col_name in tld_df.columns:\n",
    "            tld_df.at[idx, col_name] = 1\n",
    "        else:\n",
    "            tld_df.at[idx, 'tld_other'] = 1\n",
    "    return tld_df\n",
    "\n",
    "tld_dummies = process_tlds(df_features['tld'], top_tlds)\n",
    "\n",
    "# 3. PREPARE NUMERIC INPUT (X_num)\n",
    "# Drop raw 'tld' column and concatenate dummies\n",
    "X_num_raw = pd.concat([df_features.drop(['tld'], axis=1), tld_dummies], axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_num = scaler.fit_transform(X_num_raw)\n",
    "\n",
    "# 4. PREPARE TEXT INPUT (X_text)\n",
    "tokenizer = Tokenizer(char_level=True, oov_token='UNK')\n",
    "tokenizer.fit_on_texts(df['url'])\n",
    "sequences = tokenizer.texts_to_sequences(df['url'])\n",
    "max_len = 200\n",
    "X_text = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6d2d25",
   "metadata": {},
   "source": [
    "### Create Labels & Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4b90ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. PREPARE LABELS ---\n",
    "# Binary Target (for XGBoost)\n",
    "y_binary = df['malicious'].values\n",
    "\n",
    "# Multiclass Target (for Neural Network)\n",
    "encoder = LabelEncoder()\n",
    "y_multi_int = encoder.fit_transform(df['type'])\n",
    "y_multi = to_categorical(y_multi_int)\n",
    "num_classes = y_multi.shape[1]\n",
    "\n",
    "# --- 2. SPLIT DATA ---\n",
    "# We split ALL arrays at once using the same random_state to ensure alignment\n",
    "X_num_train, X_num_test, \\\n",
    "X_text_train, X_text_test, \\\n",
    "y_bin_train, y_bin_test, \\\n",
    "y_multi_train, y_multi_test = train_test_split(\n",
    "    X_num, \n",
    "    X_text, \n",
    "    y_binary, \n",
    "    y_multi, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training Samples: {len(X_num_train)}\")\n",
    "print(f\"Test Samples: {len(X_num_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10cabdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Tuning XGBoost (This will take a few minutes)...\")\n",
    "\n",
    "# 1. Define the Hyperparameter Grid (From your old file)\n",
    "param_grid_xg = {\n",
    "    'n_estimators': [100, 200], # Added 200 to see if more trees help\n",
    "    'max_depth': [3, 5, 7],     # Added 7 for deeper complexity\n",
    "    'learning_rate': [0.1, 0.05],\n",
    "    'subsample': [0.8],\n",
    "    'colsample_bytree': [0.8]\n",
    "}\n",
    "\n",
    "# 2. Set up Grid Search\n",
    "# We use 'refit=True', so after finding the best params, \n",
    "# it automatically retrains one final model on the whole X_num_train dataset.\n",
    "grid_search = GridSearchCV(\n",
    "    XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "    param_grid_xg,\n",
    "    scoring='f1',\n",
    "    cv=3, # 3-fold internal validation is enough for tuning\n",
    "    n_jobs=-1, # Use all CPU cores to speed this up\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 3. Fit on the Training Data\n",
    "grid_search.fit(X_num_train, y_bin_train)\n",
    "\n",
    "# 4. Extract the Best Model\n",
    "best_xgb = grid_search.best_estimator_\n",
    "print(f\"\\nBest Parameters Found: {grid_search.best_params_}\")\n",
    "\n",
    "# 5. Evaluate on Test Data (The data the model has never seen)\n",
    "y_pred_xgb = best_xgb.predict(X_num_test)\n",
    "\n",
    "print(\"\\nFinal XGBoost Evaluation:\")\n",
    "print(confusion_matrix(y_bin_test, y_pred_xgb))\n",
    "print(classification_report(y_bin_test, y_pred_xgb))\n",
    "\n",
    "# --- Feature Importance Visualization ---\n",
    "# We need to manually reconstruct feature names because X_num is a numpy array\n",
    "feature_names = [\n",
    "    'url_length', 'num_digits', 'num_periods', 'num_slashes', 'num_ats', \n",
    "    'digit_len_ratio', 'entropy', \n",
    "    'has_html', 'has_query_param', 'has_https', 'has_http', 'has_ip_address', \n",
    "    'has_non_ascii_chars', 'has_suspicious_kw', 'has_brand_kw', \n",
    "    'min_brand_dist', 'is_typosquat'\n",
    "]\n",
    "# Add the TLD columns we created\n",
    "feature_names += [f'tld_{t}' for t in top_tlds] + ['tld_other']\n",
    "\n",
    "# Get importances\n",
    "importances = best_xgb.feature_importances_\n",
    "\n",
    "# Sort them for a better chart\n",
    "sorted_idx = np.argsort(importances)[-20:] # Top 20 features\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh([feature_names[i] for i in sorted_idx], importances[sorted_idx])\n",
    "plt.title(\"Top 20 Feature Importance (XGBoost)\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.show()\n",
    "\n",
    "# 6. UPDATE VARIABLE FOR SAVING\n",
    "# This is crucial so the next cell saves this TUNED model, not a default one\n",
    "xgb_model = best_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3778e0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Neural Network (Multiclass)...\")\n",
    "\n",
    "def build_hybrid_model():\n",
    "    # --- Branch 1: Numerical Data ---\n",
    "    # Shape is determined automatically by X_num columns\n",
    "    input_num = Input(shape=(X_num_train.shape[1],), name='numeric_input')\n",
    "    x1 = layers.Dense(128, activation='relu')(input_num) # Increased neurons slightly\n",
    "    x1 = layers.BatchNormalization()(x1)\n",
    "    x1 = layers.Dropout(0.3)(x1)\n",
    "\n",
    "    # --- Branch 2: Text Data (Char-CNN) ---\n",
    "    input_text = Input(shape=(max_len,), name='text_input')\n",
    "    \n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    x2 = layers.Embedding(input_dim=vocab_size, output_dim=32)(input_text) # Removed input_length (deprecated)\n",
    "    \n",
    "    x2 = layers.Conv1D(filters=64, kernel_size=3, activation='relu')(x2)\n",
    "    x2 = layers.MaxPooling1D(pool_size=2)(x2)\n",
    "    \n",
    "    x2 = layers.Conv1D(filters=128, kernel_size=5, activation='relu')(x2)\n",
    "    x2 = layers.GlobalMaxPooling1D()(x2)\n",
    "    x2 = layers.Dropout(0.3)(x2)\n",
    "\n",
    "    # --- Merge ---\n",
    "    merged = concatenate([x1, x2])\n",
    "    \n",
    "    x = layers.Dense(128, activation='relu')(merged)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # Output Layer (Multiclass)\n",
    "    output = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=[input_text, input_num], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Calculate class weights for imbalance\n",
    "from sklearn.utils import class_weight\n",
    "y_multi_int_train = np.argmax(y_multi_train, axis=1)\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_multi_int_train),\n",
    "    y=y_multi_int_train\n",
    ")\n",
    "weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Build and Train\n",
    "nn_model = build_hybrid_model()\n",
    "\n",
    "early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.00001)\n",
    "\n",
    "history = nn_model.fit(\n",
    "    [X_text_train, X_num_train], # Two inputs!\n",
    "    y_multi_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=20, # 20 is usually enough with early stopping\n",
    "    batch_size=32, \n",
    "    callbacks=[early_stop, lr_scheduler], \n",
    "    class_weight=weights_dict,\n",
    "    verbose=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
