{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlcroissant as mlc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# feature engineering\n",
    "import tldextract\n",
    "import Levenshtein\n",
    "\n",
    "# logistic regression from sklearn\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import KFold, cross_val_predict, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, mean_squared_error, r2_score,\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_curve, auc, classification_report\n",
    ")\n",
    "\n",
    "# svm and decision tree from sklearn\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import math\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# XGBoost and Random Forest\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Neural network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, Input, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization, concatenate\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the Croissant JSON-LD\n",
    "croissant_dataset = mlc.Dataset('https://www.kaggle.com/datasets/naveenbhadouria/malicious/croissant/download')\n",
    "\n",
    "# Check what record sets are in the dataset\n",
    "record_sets = croissant_dataset.metadata.record_sets\n",
    "print(record_sets)\n",
    "\n",
    "# Fetch the records and put them in a DataFrame\n",
    "record_set_df = pd.DataFrame(croissant_dataset.records(record_set=record_sets[0].uuid))\n",
    "df = record_set_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "df = df.rename(columns={'malicious_phish1.csv/url' : 'url', 'malicious_phish1.csv/type' : 'type'})\n",
    "\n",
    "# Use errors='replace' to substitute bad bytes with a placeholder\n",
    "df['url'] = df['url'].apply(lambda x: x.decode('utf-8', errors='replace') if isinstance(x, bytes) else x)\n",
    "df['type'] = df['type'].apply(lambda x: x.decode('utf-8', errors='replace') if isinstance(x, bytes) else x)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['malicious'] = (df['type'] != 'benign').astype(int)\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "# df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "#### Dataset Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['type'].value_counts().plot(kind='bar', figsize=(7, 4), color='blue')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Distribution of URL Types', fontsize=16)\n",
    "plt.xlabel('Type', fontsize=10)\n",
    "plt.ylabel('Count', fontsize=10)\n",
    "plt.grid(True, linestyle=':', alpha=0.7)\n",
    "plt.xticks(rotation=0) # Keeps the x-axis labels horizontal\n",
    "plt.show()\n",
    "\n",
    "label_map = {0: 'Benign', 1: 'Malicious'}\n",
    "\n",
    "df['malicious'].map(label_map).value_counts().plot(\n",
    "    kind='bar', \n",
    "    figsize=(7, 4), \n",
    "    color='blue'\n",
    ")\n",
    "\n",
    "plt.title('Distribution of Safe vs. Malicious URLs', fontsize=16)\n",
    "plt.xlabel('Type', fontsize=10)\n",
    "plt.ylabel('Count', fontsize=10)\n",
    "plt.grid(True, linestyle=':', alpha=0.7)\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "#### Length Based Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of URL\n",
    "df['url_length'] = df['url'].str.len()\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "#### Count Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of digits\n",
    "df['num_digits'] = df['url'].str.count(r'\\d')\n",
    "\n",
    "# Number of periods\n",
    "df['num_periods'] = df['url'].str.count(r'\\.')\n",
    "\n",
    "# Number of slashes\n",
    "df['num_slashes'] = df['url'].str.count(r'\\/')\n",
    "\n",
    "# Number of @ symbols\n",
    "df['num_ats'] = df['url'].str.count(r'\\@')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "#### Binary Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for advanced features\n",
    "\n",
    "# Extract domain name (google.com, amazon.com, etc.)\n",
    "def extract_domain(url):\n",
    "    extracted = tldextract.extract(url)\n",
    "    return extracted.domain\n",
    "\n",
    "df['domain'] = df['url'].apply(extract_domain)\n",
    "\n",
    "\n",
    "# List of brands\n",
    "brands = ['google', 'paypal', 'microsoft', 'apple', 'amazon', 'netflix', 'facebook']\n",
    "\n",
    "# Typosquat\n",
    "def get_typosquat_feature(url_domain):\n",
    "    min_dist = 100 \n",
    "\n",
    "    clean_domain = url_domain.lower()\n",
    "    \n",
    "    for brand in brands:\n",
    "        dist = Levenshtein.distance(clean_domain, brand)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            \n",
    "    return min_dist\n",
    "\n",
    "df['min_brand_dist'] = df['domain'].apply(get_typosquat_feature)\n",
    "\n",
    "# Flag domains that are close to a brand name but not exact\n",
    "def categorize_typosquat(dist):\n",
    "    # if dist = 0 its the real brand\n",
    "    if dist == 0:\n",
    "        return 0 \n",
    "    # flag if its close to the brand name but not exact (go0gle has dist 1)\n",
    "    elif 0 < dist <= 2:\n",
    "        return 1 \n",
    "    # ignore if not close at all\n",
    "    else:\n",
    "        return 0 \n",
    "\n",
    "df['is_typosquat'] = df['min_brand_dist'].apply(categorize_typosquat)\n",
    "\n",
    "# ratio of digits to total length - high ratio is more suspicious\n",
    "df['digit_len_ratio'] = df['num_digits'] / df['url_length']\n",
    "\n",
    "# Suspicious brand word usage\n",
    "def check_brand_usage(row):\n",
    "    url_str = str(row['url']).lower()\n",
    "    domain_str = str(row['domain']).lower()\n",
    "    \n",
    "    for brand in brands:\n",
    "        # Check if brand name appears in the URL path/subdomain...\n",
    "        if brand in url_str:\n",
    "            # ...but is NOT the actual domain name (e.g. paypal-login.com)\n",
    "            if brand not in domain_str:\n",
    "                return 1 # Suspicious!\n",
    "    return 0\n",
    "\n",
    "df['is_suspicious_brand_usage'] = df.apply(check_brand_usage, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure URL column is string\n",
    "df['url'] = df['url'].astype(str)\n",
    "\n",
    "# Has \".htm\" or \".html\"\n",
    "df['has_html'] = df['url'].str.contains(r'\\.html?', case=False, na=False).astype(int)\n",
    "\n",
    "# Has literal \"?query=\"\n",
    "df['has_query_param'] = df['url'].str.contains(r'\\?query=', case=False, na=False).astype(int)\n",
    "\n",
    "# Has \"https://\"\n",
    "df['has_https'] = df['url'].str.contains(r'^https://', case=False, na=False).astype(int)\n",
    "\n",
    "# Has \"http://\" (less secure)\n",
    "df['has_http'] = df['url'].str.contains(r'^http://', case=False, na=False).astype(int)\n",
    "\n",
    "# Has IPv4 address in the host part (after ://)\n",
    "df['has_ip_address'] = df['url'].str.contains(r'://(?:\\d{1,3}\\.){3}\\d{1,3}', case=False, na=False).astype(int)\n",
    "\n",
    "# Suspicious keywords anywhere in URL\n",
    "suspicious_kw = {'login', 'secure', 'payment', 'verify'}\n",
    "pattern = '(' + '|'.join(suspicious_kw) + ')'\n",
    "df['has_suspicious_kw'] = df['url'].str.contains(pattern, case=False, na=False).astype(int)\n",
    "\n",
    "# Brand keywords\n",
    "df['has_brand_kw'] = df['url'].str.contains(r'(google|paypal|microsoft)', case=False, na=False).astype(int)\n",
    "\n",
    "# Has non ascii characters - mixed scripts\n",
    "df['has_non_ascii_chars'] = df['url'].str.contains(r'[^\\x00-\\x7F]', regex=True).astype(int)\n",
    "\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "# Model Training and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "#### X, y setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup X and y and scale X\n",
    "y = df['malicious']\n",
    "X = df.drop(['url', 'malicious', 'type', 'domain', 'users'], axis=1, errors='ignore')\n",
    "\n",
    "# scaled X\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "#### Logistic Regression (K-fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(max_iter=1000)     #maximum number of iterations for optimization.\n",
    "\n",
    "# create kf as a K-Fold cross validator and relevant indicies cv_split\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_split = kf.split(X_scaled)\n",
    "\n",
    "# fit the logistic regression model log_reg to the split X and y training sets and make prediction\n",
    "log_reg = LogisticRegression(max_iter=1000) \n",
    "log_reg.fit(X_scaled,y)\n",
    "y_pred = cross_val_predict(log_reg, X_scaled, y, cv=cv_split)\n",
    "\n",
    "# score evaluation\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "#### Logistic Regression (Train-Test Split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into 80/20 train to test split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled,y, test_size = .2, random_state = 42)\n",
    "\n",
    "# fit the logistic regression model log_reg to the split X and y training sets and make prediction\n",
    "log_reg.fit(X_train,y_train)\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# score evaluation\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for trees\n",
    "\n",
    "# function to return F1, Accuracy, Precision, and Recall\n",
    "# return: dict containing metrics\n",
    "def metrics_from_cm(cm):\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    prec = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "    rec  = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "    f1   = 2*prec*rec / (prec + rec) if (prec + rec) else 0.0\n",
    "    acc  = (tp + tn) / cm.sum() if cm.sum() else 0.0\n",
    "    return dict(precision=prec, recall=rec, f1=f1, accuracy=acc)\n",
    "\n",
    "# function to get the best hyperparameter combo from a grid search and print the results\n",
    "def print_best_results(results):\n",
    "    # extract best f1 score combination and print \n",
    "    best = max(results, key=lambda r: r['f1'])\n",
    "\n",
    "    if (best['model'] == 'SVM'):\n",
    "        if (best['kernel'] == 'rbf'):\n",
    "            svm_best_params = {'kernel': best['kernel'],\n",
    "                               'C': best['C'],\n",
    "                               'gamma': best['gamma']}\n",
    "        else:\n",
    "            svm_best_params = {'kernel': best['kernel'],\n",
    "                                'C': best['C']}\n",
    "    else:\n",
    "        svm_best_params = {'max depth': best['max_depth'], \n",
    "                           'criterion': best['criterion'], \n",
    "                           'min_samples_split': best['min_samples_split'],}\n",
    "\n",
    "    svm_best_f1 = best['f1']\n",
    "    svm_cm = best['cm']\n",
    "\n",
    "    print(\"Best Hyperparameters:\", svm_best_params)\n",
    "    print(\"Best F1 Score:\", round(svm_best_f1, 4))\n",
    "    print(\"Confusion Matrix:\\n\", svm_cm, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "#### Support Vector Machine (Train-Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # need to make a smaller dataset since SVC creates an n x n array\n",
    "# X_small = X[:len(X)//10]\n",
    "# y_small = y[:len(y)//10]\n",
    "\n",
    "# # proceed as usual\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X_small, y_small,\n",
    "#     test_size=0.2,\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# results = []\n",
    "\n",
    "# for C in [0.1, 1, 10]:\n",
    "#     clf = SVC(C=C, kernel='linear')\n",
    "#     clf.fit(X_train, y_train)\n",
    "#     y_pred= clf.predict(X_test)\n",
    "#     cm = confusion_matrix(y_test, y_pred, labels=[0, 1])\n",
    "    \n",
    "#     cm = confusion_matrix(y_test, y_pred, labels=[0, 1])\n",
    "#     mets = metrics_from_cm(cm)\n",
    "#     results.append({\n",
    "#         'model': 'LinearSVM', \n",
    "#         'C': C, \n",
    "#         'cm': cm, \n",
    "#         **mets})\n",
    "\n",
    "# print_best_results(results=results)\n",
    "\n",
    "# need to make a smaller dataset since SVC creates an n x n array\n",
    "X_small = X[:len(X)//10]\n",
    "y_small = y[:len(y)//10]\n",
    "\n",
    "# proceed as usual\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small, y_small,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "results = []\n",
    "\n",
    "svm_params = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "for C in svm_params['C']:\n",
    "    for kernel in svm_params['kernel']:\n",
    "        gammas = [None] if kernel == 'linear' else svm_params['gamma']\n",
    "        for gamma in gammas:\n",
    "\n",
    "            clf = SVC(C=C, kernel=kernel)\n",
    "            if (kernel == 'rbf'):\n",
    "                clf.set_params(gamma=gamma)\n",
    "\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred= clf.predict(X_test)\n",
    "            cm = confusion_matrix(y_test, y_pred, labels=[0, 1])\n",
    "            \n",
    "            cm = confusion_matrix(y_test, y_pred, labels=[0, 1])\n",
    "            mets = metrics_from_cm(cm)\n",
    "            results.append({\n",
    "                'model': 'SVM', \n",
    "                'C': C, \n",
    "                'kernel': kernel,\n",
    "                'gamma': gamma,\n",
    "                'cm': cm, \n",
    "                **mets})\n",
    "\n",
    "print_best_results(results=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to make a smaller dataset since SVC creates an n x n array\n",
    "X_small = X[:len(X)//10]\n",
    "y_small = y[:len(y)//10]\n",
    "\n",
    "# proceed as usual\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small, y_small,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "results = []\n",
    "\n",
    "for C in [0.1, 1, 10]:\n",
    "    clf = SVC(C=C, kernel='linear')\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred= clf.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=[0, 1])\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred, labels=[0, 1])\n",
    "    mets = metrics_from_cm(cm)\n",
    "    results.append({\n",
    "        'model': 'LinearSVM', \n",
    "        'C': C, \n",
    "        'cm': cm, \n",
    "        **mets})\n",
    "\n",
    "print_best_results(results=results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "#### SVM (K-Fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to make a smaller dataset since SVC creates an n x n array\n",
    "X_small = X[:len(X)//10]\n",
    "y_small = y[:len(y)//10]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_small_scaled = scaler.fit_transform(X_small)\n",
    "\n",
    "# declare k-fold object \n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "results = []\n",
    "\n",
    "for C in [0.1, 1, 10]:\n",
    "    # Train model\n",
    "    clf = SVC(C=C, kernel='linear')\n",
    "    y_pred = cross_val_predict(clf, X_small_scaled, y_small, cv=kf)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    cm = confusion_matrix(y_small, y_pred, labels=[0, 1])\n",
    "    mets = metrics_from_cm(cm)\n",
    "    results.append({\n",
    "        'model': 'LinearSVM', \n",
    "        'C': C, \n",
    "        'cm': cm, \n",
    "        **mets\n",
    "    })\n",
    "\n",
    "print_best_results(results=results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "#### Decision Tree (Test-Train Split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set test and train data \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42)\n",
    "\n",
    "# scale test and train\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# tree hyperparameters\n",
    "tree_params = {\n",
    "    'max_depth': [3,5],\n",
    "    'criterion': ['entropy'],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "# Loop through all SVM parameters\n",
    "for max_depth in tree_params['max_depth']:\n",
    "    for criterion in tree_params['criterion']:\n",
    "        for min_samples_split in tree_params['min_samples_split']:\n",
    "            \n",
    "            # create SVC object with given parameters\n",
    "            clf = DecisionTreeClassifier(\n",
    "                max_depth=max_depth,\n",
    "                criterion=criterion,\n",
    "                min_samples_split=min_samples_split\n",
    "            )\n",
    "\n",
    "            # fit object on training data and generate prediction\n",
    "            clf.fit(X_train, y_train)\n",
    "            yp = clf.predict(X_test)\n",
    "\n",
    "            # add prediction to cm\n",
    "            cm = confusion_matrix(y_test, yp, labels=[0, 1])\n",
    "            mets = metrics_from_cm(cm)\n",
    "            results.append({\n",
    "                'model': 'Decision Tree', 'max_depth': max_depth, 'criterion': criterion, 'min_samples_split': min_samples_split,\n",
    "                'cm': cm, **mets\n",
    "            })\n",
    "\n",
    "print_best_results(results=results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "#### Decision Tree (K-Fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree hyperparameters\n",
    "tree_params = {\n",
    "    'max_depth': [3,5],\n",
    "    'criterion': ['entropy'],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "# create kf as a K-Fold cross validator and relevant indicies cv_split\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_split = kf.split(X_scaled)\n",
    "\n",
    "# Loop through all SVM parameters\n",
    "for max_depth in tree_params['max_depth']:\n",
    "    for criterion in tree_params['criterion']:\n",
    "        for min_samples_split in tree_params['min_samples_split']:\n",
    "\n",
    "            clf = DecisionTreeClassifier(\n",
    "                max_depth=max_depth,\n",
    "                criterion=criterion,\n",
    "                min_samples_split=min_samples_split\n",
    "            )\n",
    "\n",
    "            # cross_val_predict trains internally â€“ no clf.fit() here\n",
    "            y_pred = cross_val_predict(clf, X_scaled, y, cv=kf)\n",
    "\n",
    "            # Compare to full y (not y_test)\n",
    "            cm = confusion_matrix(y, y_pred, labels=[0, 1])\n",
    "            mets = metrics_from_cm(cm)\n",
    "\n",
    "            results.append({\n",
    "                'model': 'Decision Tree', 'max_depth': max_depth, 'criterion': criterion, 'min_samples_split': min_samples_split,\n",
    "                'cm': cm, **mets\n",
    "            })\n",
    "    \n",
    "print_best_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "#### XGBoost (K-Fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid\n",
    "param_grid_xg = {\n",
    "    'n_estimators': [100],\n",
    "    'max_depth': [3],\n",
    "    'learning_rate': [0.1],\n",
    "    'subsample': [0.8],\n",
    "    'colsample_bytree': [0.8]\n",
    "}\n",
    "\n",
    "# K-fold cross-validation \n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "y_true_xg = []\n",
    "y_pred_xg = []\n",
    "\n",
    "for train_idx, test_idx in kf.split(X, y):\n",
    "    # set training and test X and y\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "    # Grid search on the training fold\n",
    "    model = GridSearchCV(\n",
    "        XGBClassifier(),\n",
    "        param_grid_xg,\n",
    "        scoring='f1',\n",
    "        cv=3\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # use best estimator from grid search\n",
    "    best_xgb = model.best_estimator_\n",
    "    pred = best_xgb.predict(X_test)\n",
    "\n",
    "    # collect predictions and true labels\n",
    "    y_true_xg.extend(y_test.values)\n",
    "    y_pred_xg.extend(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true_xg, y_pred_xg))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true_xg, y_pred_xg))\n",
    "\n",
    "# Feature Importance\n",
    "best_model = model.best_estimator_ #get hyperparameter combination for the best model.\n",
    "importances = best_model.feature_importances_\n",
    "features = X.columns\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(features, importances)\n",
    "plt.title(\"Feature Importance (XGBoost)\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Importance Score\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "#### Random Forest (K-Fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 150],\n",
    "    'max_depth': [5, 10],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]   \n",
    "}\n",
    "\n",
    "# K-fold cross-validation \n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for train_idx, test_idx in kf.split(X, y):\n",
    "    # set training and test X and y\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "    # Grid search on the training fold\n",
    "    model = GridSearchCV(RandomForestClassifier(),\n",
    "                         param_grid, \n",
    "                         scoring='f1',  # Use F1 score for evaluation\n",
    "                         cv=3)\n",
    "    \n",
    "     # fit model object on training data and make prediction\n",
    "    model.fit(X_train, y_train)       \n",
    "    pred = model.predict(X_test)\n",
    "\n",
    "    # add predictiction and corresponding actual value to arrays\n",
    "    y_true.extend(y_test.values)     \n",
    "    y_pred.extend(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Feature Importance\n",
    "best_model = model.best_estimator_\n",
    "importances = best_model.feature_importances_\n",
    "features = X.columns\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(features, importances)\n",
    "plt.title(\"Feature Importance (Random Forest)\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Importance Score\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **** SAVED DATA AS CSV TO SPEED UP PROCESSING ****    \n",
    "df = pd.read_csv('processed_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "##### Initialize Features - Numerical and Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUMERICAL FEATURES\n",
    "# Calculate Shannon entropy - measures how random the url is (if its gibberish its probably bad)\n",
    "def calc_entropy(text):\n",
    "    if not text: return 0\n",
    "    entropy = 0\n",
    "    for x, n in Counter(str(text)).items():\n",
    "        p = n / len(text)\n",
    "        entropy -= p * math.log2(p)\n",
    "    return entropy\n",
    "\n",
    "df['entropy'] = df['url'].apply(calc_entropy)\n",
    "\n",
    "# Extract TLD \n",
    "df['tld'] = df['url'].apply(lambda x: x.split('.')[-1].split('/')[0])\n",
    "top_tlds = df['tld'].value_counts().nlargest(20).index\n",
    "df['tld_processed'] = df['tld'].apply(lambda x: x if x in top_tlds else 'other')\n",
    "tld_dummies = pd.get_dummies(df['tld_processed'], prefix='tld')\n",
    "\n",
    "# Drop non-numerical columns\n",
    "drop_cols = ['url', 'type', 'malicious', 'domain', 'users', 'min_brand_dist', 'is_typosquat', 'tld', 'tld_processed']\n",
    "numeric_cols = [c for c in df.columns if c not in drop_cols]\n",
    "X_num_raw = df[numeric_cols]\n",
    "\n",
    "# Join TLD features to numerical features\n",
    "X_num_raw = pd.concat([X_num_raw, tld_dummies], axis=1)\n",
    "\n",
    "# Scale numerical data\n",
    "scaler = StandardScaler()\n",
    "X_num = scaler.fit_transform(X_num_raw)\n",
    "\n",
    "# TEXT FEATURES \n",
    "\n",
    "# Character-level Tokenization \n",
    "# Instead of each word being a token we treat the URL as a sequence of characters\n",
    "tokenizer = Tokenizer(char_level=True, oov_token='UNK')\n",
    "tokenizer.fit_on_texts(df['url'])\n",
    "sequences = tokenizer.texts_to_sequences(df['url'])\n",
    "\n",
    "# Pad sequences\n",
    "max_len = 200 \n",
    "X_text = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# LABELS AND SPLIT\n",
    "\n",
    "y_raw = df['type']\n",
    "encoder = LabelEncoder()\n",
    "y_int = encoder.fit_transform(y_raw)\n",
    "y_categorical = to_categorical(y_int)\n",
    "num_classes = y_categorical.shape[1]\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_int),\n",
    "    y=y_int\n",
    ")\n",
    "weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Split data\n",
    "X_text_train, X_text_test, y_train, y_test = train_test_split(X_text, y_categorical, test_size=0.2, random_state=42)\n",
    "X_num_train, X_num_test, _, _ = train_test_split(X_num, y_categorical, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "##### Build and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hybrid_model():\n",
    "    # Part 1 - numerical data\n",
    "    input_num = Input(shape=(X_num_train.shape[1],), name='numeric_input')\n",
    "    x1 = layers.Dense(64, activation='relu')(input_num)\n",
    "    x1 = layers.BatchNormalization()(x1)\n",
    "    x1 = layers.Dropout(0.3)(x1)\n",
    "\n",
    "    # Part 2 - Character embedding + CNN\n",
    "    input_text = Input(shape=(max_len,), name='text_input')\n",
    "    \n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    # Embedding layer\n",
    "    x2 = layers.Embedding(input_dim=vocab_size, output_dim=32, input_length=max_len)(input_text)\n",
    "    \n",
    "    # Conv1D layers\n",
    "    x2 = layers.Conv1D(filters=64, kernel_size=3, activation='relu')(x2)\n",
    "    x2 = layers.MaxPooling1D(pool_size=2)(x2)\n",
    "    \n",
    "    x2 = layers.Conv1D(filters=128, kernel_size=5, activation='relu')(x2)\n",
    "    x2 = layers.GlobalMaxPooling1D()(x2)\n",
    "    x2 = layers.Dropout(0.3)(x2)\n",
    "\n",
    "    # Merge numeric and text features\n",
    "    merged = concatenate([x1, x2])\n",
    "    \n",
    "    x = layers.Dense(128, activation='relu')(merged)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    output = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=[input_text, input_num], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "print(\"\\nTraining Neural Network\")\n",
    "model = build_hybrid_model()\n",
    "\n",
    "early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.5,       \n",
    "    patience=2,       \n",
    "    min_lr=0.00001,   \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    [X_text_train, X_num_train], \n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=32, \n",
    "    callbacks=[early_stop, lr_scheduler], \n",
    "    class_weight=weights_dict,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "loss, accuracy = model.evaluate([X_text_test, X_num_test], y_test, verbose=0)\n",
    "print(f\"Final Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "##### Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_probs = model.predict([X_text_test, X_num_test])\n",
    "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(classification_report(y_true_classes, y_pred_classes, target_names=encoder.classes_))\n",
    "\n",
    "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=encoder.classes_, \n",
    "            yticklabels=encoder.classes_)\n",
    "plt.title('Hybrid Model (Text + Numerical) Results')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xgb311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
